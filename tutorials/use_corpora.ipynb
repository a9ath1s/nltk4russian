{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Использование текстовых корпусов с помощью Natural Language Toolkit (nltk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этой тетрадке описываются некоторые полезные функции, позволяющие работать с корпусами, доступными из nltk, и пользовательскими корпусами.\n",
    "Желательно заранее создать папку с текстовыми файлами (хотя бы одним) или использовать [файлы из репозитория]().\n",
    "\n",
    "<span style=\"color:red\">*NB!*</span> Не забудьте сохранить файлы в кодировке utf-8 на Windows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Начинаем с импорта библиотеки nltk.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "# raw corpora\n",
    "from nltk.corpus import gutenberg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Второй строчкой мы импортировали один из специальных модулей, который читает корпус электронной библиотеки проекта Гутенберг. Теперь попробуем посмотреть, какие тексты есть в этом корпусе. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если списка файлов нет, значит, соответствующие корпуса для nltk не загружены. Это можно сделать следующим образом (проверьте наличие интернет-соединения!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В появившемся окне можно выбрать `all-corpora` во вкладке **Collections** или отдельные корпуса во вкладке **Corpora**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь попробуем получить список слов для одного из текстов и запишем его в переменную `alice`.\n",
    "В скобках можно написать имя любого файла  из предыдущего списка. Также можно перечислить несколько имён в квадратных скобках (`[file1, file2, file3]`) или не указывать ничего - тогда мы получим список слов всего корпуса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alice = nltk.corpus.gutenberg.words('carroll-alice.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат работы функции `words()` является списком (см. [документацию](https://docs.python.org/3/tutorial/datastructures.html)), поэтому над ним можно производить все соответствующие операции. Например, вычислить длину (количество входящих в него элементов):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34110"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(alice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Или вывести первые несколько слов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Alice ' s Adventures in Wonderland by Lewis Carroll 1865 ] CHAPTER I . Down the Rabbit - Hole\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(alice[:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким же образом можно получить список предложений с помощью функции `sents()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alice_sents = nltk.corpus.gutenberg.sents('carroll-alice.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Элементами списка предложений являются списки слов; мы так же можем вывести их (в примере для наглядности после каждого предложения выведем пустую строку):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Alice ' s Adventures in Wonderland by Lewis Carroll 1865 ]\n",
      "\n",
      "CHAPTER I .\n",
      "\n",
      "Down the Rabbit - Hole\n",
      "\n",
      "Alice was beginning to get very tired of sitting by her sister on the bank , and of having nothing to do : once or twice she had peeped into the book her sister was reading , but it had no pictures or conversations in it , ' and what is the use of a book ,' thought Alice ' without pictures or conversation ?'\n",
      "\n",
      "So she was considering in her own mind ( as well as she could , for the hot day made her feel very sleepy and stupid ), whether the pleasure of making a daisy - chain would be worth the trouble of getting up and picking the daisies , when suddenly a White Rabbit with pink eyes ran close by her .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for s in alice_sents[:5]:\n",
    "    print(' '.join(s))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Размеченные корпуса**\n",
    "\n",
    "Помимо корпусов в формате plain-text с помощью nltk можно работать с корпусами, содержащими лингвистическую разметку. Например, это [Brown corpus](https://en.wikipedia.org/wiki/Brown_Corpus):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# categorized corpora\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Корпус содержит метаразметку: указаны жанры текстов. Их можно посмотреть с помощью функции `categories()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.brown.categories()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее эти категории можно использовать, чтобы работать с определённым подкорпусом. Например, можно снова вывести список слов, но для заданной категории:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.brown.words(categories='news')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для корпусов с разметкой можно воспользоваться функциями `tagged_words()` и `tagged_sents()`, чтобы получить слова/пердложения с морфологическими тегами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'AT'),\n",
       " ('Fulton', 'NP-TL'),\n",
       " ('County', 'NN-TL'),\n",
       " ('Grand', 'JJ-TL'),\n",
       " ('Jury', 'NN-TL'),\n",
       " ('said', 'VBD'),\n",
       " ('Friday', 'NR'),\n",
       " ('an', 'AT'),\n",
       " ('investigation', 'NN'),\n",
       " ('of', 'IN'),\n",
       " (\"Atlanta's\", 'NP$'),\n",
       " ('recent', 'JJ'),\n",
       " ('primary', 'NN'),\n",
       " ('election', 'NN'),\n",
       " ('produced', 'VBD'),\n",
       " ('``', '``'),\n",
       " ('no', 'AT'),\n",
       " ('evidence', 'NN'),\n",
       " (\"''\", \"''\"),\n",
       " ('that', 'CS')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.brown.tagged_words()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'AT'),\n",
       " ('Fulton', 'NP-TL'),\n",
       " ('County', 'NN-TL'),\n",
       " ('Grand', 'JJ-TL'),\n",
       " ('Jury', 'NN-TL'),\n",
       " ('said', 'VBD'),\n",
       " ('Friday', 'NR'),\n",
       " ('an', 'AT'),\n",
       " ('investigation', 'NN'),\n",
       " ('of', 'IN'),\n",
       " (\"Atlanta's\", 'NP$'),\n",
       " ('recent', 'JJ'),\n",
       " ('primary', 'NN'),\n",
       " ('election', 'NN'),\n",
       " ('produced', 'VBD'),\n",
       " ('``', '``'),\n",
       " ('no', 'AT'),\n",
       " ('evidence', 'NN'),\n",
       " (\"''\", \"''\"),\n",
       " ('that', 'CS'),\n",
       " ('any', 'DTI'),\n",
       " ('irregularities', 'NNS'),\n",
       " ('took', 'VBD'),\n",
       " ('place', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.brown.tagged_sents()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По умолчанию выводится разметка в формате Брауновского корпуса; можно использовать тегсет [Universal Dependencies](http://universaldependencies.org/u/pos/all.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DET'),\n",
       " ('Fulton', 'NOUN'),\n",
       " ('County', 'NOUN'),\n",
       " ('Grand', 'ADJ'),\n",
       " ('Jury', 'NOUN'),\n",
       " ('said', 'VERB'),\n",
       " ('Friday', 'NOUN'),\n",
       " ('an', 'DET'),\n",
       " ('investigation', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " (\"Atlanta's\", 'NOUN'),\n",
       " ('recent', 'ADJ'),\n",
       " ('primary', 'NOUN'),\n",
       " ('election', 'NOUN'),\n",
       " ('produced', 'VERB'),\n",
       " ('``', '.'),\n",
       " ('no', 'DET'),\n",
       " ('evidence', 'NOUN'),\n",
       " (\"''\", '.'),\n",
       " ('that', 'ADP')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.brown.tagged_words(tagset=\"universal\")[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Пользовательские корпуса**\n",
    "\n",
    "Похожим образом можно использовать свои корпуса; достаточно указать путь к папке с текстовыми файлами следующим образом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user corpora\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "corpus_root = '../data/'\n",
    "# второй параметр в скобках - регулярное выражение, описывающее файлы корпуса\n",
    "wordlists = PlaintextCorpusReader(corpus_root, '.*.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно так же посмотреть список файлов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input.txt']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordlists.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Список слов и предложений:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Фильм', 'Оливера', 'Стоуна', '\"', 'Александр', '\"', ...]\n"
     ]
    }
   ],
   "source": [
    "print(wordlists.words('input.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Фильм Оливера Стоуна \" Александр \" основан на реальной жизни одного из самых выдающихся людей в истории .\n",
      "\n",
      "\" Титаник \" ( Titanic ) — фильм - катастрофа 1997 года , снятый Джеймсом Кэмероном , в котором показана гибель легендарного лайнера « Титаник ».\n",
      "\n",
      "Главные роли в фильме исполнили Кейт Уинслет ( Роза Дьюитт Бьюкейтер ) и Леонардо Ди Каприо ( Джек Доусон ).\n",
      "\n",
      "« Неприкасаемые »( Intouchables ) — трагикомедийный фильм 2011 года , основанный на реальных событиях .\n",
      "\n",
      "Главные роли исполняют Франсуа Клюзе и Омар Си , удостоенный за эту актёрскую работу национальной премии « Сезар ».\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for s in wordlists.sents()[:5]:\n",
    "    print(' '.join(s))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно также загрузить корпус как `nltk.Text`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = wordlists.words()\n",
    "text = nltk.Text(w.lower() for w in words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вот так посмотрим на методы, которые можно использовать при работе с таким объектом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Text in module nltk.text object:\n",
      "\n",
      "class Text(builtins.object)\n",
      " |  A wrapper around a sequence of simple (string) tokens, which is\n",
      " |  intended to support initial exploration of texts (via the\n",
      " |  interactive console).  Its methods perform a variety of analyses\n",
      " |  on the text's contexts (e.g., counting, concordancing, collocation\n",
      " |  discovery), and display the results.  If you wish to write a\n",
      " |  program which makes use of these analyses, then you should bypass\n",
      " |  the ``Text`` class, and use the appropriate analysis function or\n",
      " |  class directly instead.\n",
      " |  \n",
      " |  A ``Text`` is typically initialized from a given document or\n",
      " |  corpus.  E.g.:\n",
      " |  \n",
      " |  >>> import nltk.corpus\n",
      " |  >>> from nltk.text import Text\n",
      " |  >>> moby = Text(nltk.corpus.gutenberg.words('melville-moby_dick.txt'))\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getitem__(self, i)\n",
      " |  \n",
      " |  __init__(self, tokens, name=None)\n",
      " |      Create a Text object.\n",
      " |      \n",
      " |      :param tokens: The source text.\n",
      " |      :type tokens: sequence of str\n",
      " |  \n",
      " |  __len__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |  \n",
      " |  __str__(self)\n",
      " |  \n",
      " |  __unicode__ = __str__(self)\n",
      " |  \n",
      " |  collocations(self, num=20, window_size=2)\n",
      " |      Print collocations derived from the text, ignoring stopwords.\n",
      " |      \n",
      " |      :seealso: find_collocations\n",
      " |      :param num: The maximum number of collocations to print.\n",
      " |      :type num: int\n",
      " |      :param window_size: The number of tokens spanned by a collocation (default=2)\n",
      " |      :type window_size: int\n",
      " |  \n",
      " |  common_contexts(self, words, num=20)\n",
      " |      Find contexts where the specified words appear; list\n",
      " |      most frequent common contexts first.\n",
      " |      \n",
      " |      :param word: The word used to seed the similarity search\n",
      " |      :type word: str\n",
      " |      :param num: The number of words to generate (default=20)\n",
      " |      :type num: int\n",
      " |      :seealso: ContextIndex.common_contexts()\n",
      " |  \n",
      " |  concordance(self, word, width=79, lines=25)\n",
      " |      Print a concordance for ``word`` with the specified context window.\n",
      " |      Word matching is not case-sensitive.\n",
      " |      :seealso: ``ConcordanceIndex``\n",
      " |  \n",
      " |  count(self, word)\n",
      " |      Count the number of times this word appears in the text.\n",
      " |  \n",
      " |  dispersion_plot(self, words)\n",
      " |      Produce a plot showing the distribution of the words through the text.\n",
      " |      Requires pylab to be installed.\n",
      " |      \n",
      " |      :param words: The words to be plotted\n",
      " |      :type words: list(str)\n",
      " |      :seealso: nltk.draw.dispersion_plot()\n",
      " |  \n",
      " |  findall(self, regexp)\n",
      " |      Find instances of the regular expression in the text.\n",
      " |      The text is a list of tokens, and a regexp pattern to match\n",
      " |      a single token must be surrounded by angle brackets.  E.g.\n",
      " |      \n",
      " |      >>> print('hack'); from nltk.book import text1, text5, text9\n",
      " |      hack...\n",
      " |      >>> text5.findall(\"<.*><.*><bro>\")\n",
      " |      you rule bro; telling you bro; u twizted bro\n",
      " |      >>> text1.findall(\"<a>(<.*>)<man>\")\n",
      " |      monied; nervous; dangerous; white; white; white; pious; queer; good;\n",
      " |      mature; white; Cape; great; wise; wise; butterless; white; fiendish;\n",
      " |      pale; furious; better; certain; complete; dismasted; younger; brave;\n",
      " |      brave; brave; brave\n",
      " |      >>> text9.findall(\"<th.*>{3,}\")\n",
      " |      thread through those; the thought that; that the thing; the thing\n",
      " |      that; that that thing; through these than through; them that the;\n",
      " |      through the thick; them that they; thought that the\n",
      " |      \n",
      " |      :param regexp: A regular expression\n",
      " |      :type regexp: str\n",
      " |  \n",
      " |  generate(self, words)\n",
      " |      Issues a reminder to users following the book online\n",
      " |  \n",
      " |  index(self, word)\n",
      " |      Find the index of the first occurrence of the word in the text.\n",
      " |  \n",
      " |  plot(self, *args)\n",
      " |      See documentation for FreqDist.plot()\n",
      " |      :seealso: nltk.prob.FreqDist.plot()\n",
      " |  \n",
      " |  readability(self, method)\n",
      " |  \n",
      " |  similar(self, word, num=20)\n",
      " |      Distributional similarity: find other words which appear in the\n",
      " |      same contexts as the specified word; list most similar words first.\n",
      " |      \n",
      " |      :param word: The word used to seed the similarity search\n",
      " |      :type word: str\n",
      " |      :param num: The number of words to generate (default=20)\n",
      " |      :type num: int\n",
      " |      :seealso: ContextIndex.similar_words()\n",
      " |  \n",
      " |  unicode_repr = __repr__(self)\n",
      " |  \n",
      " |  vocab(self)\n",
      " |      :seealso: nltk.prob.FreqDist\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на некоторые из этих функций (кажется, описания выше можно не дублировать):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('\"', 4),\n",
       " (',', 4),\n",
       " ('в', 3),\n",
       " ('«', 3),\n",
       " ('фильм', 3),\n",
       " (')', 3),\n",
       " ('(', 3),\n",
       " ('».', 2),\n",
       " ('—', 2),\n",
       " ('и', 2)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.vocab().most_common(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "главные роли\n"
     ]
    }
   ],
   "source": [
    "text.collocations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Токенизация и теггинг в nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В nltk есть встроенные функции токенизации и разметки морфологическими тегами. Для морфоразметки используется один из встроенных теггеров nltk (для английского языка - `PerceptronTagger`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('There', 'EX'),\n",
       " ('is', 'VBZ'),\n",
       " ('some', 'DT'),\n",
       " ('text', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('split', 'VB'),\n",
       " ('into', 'IN'),\n",
       " ('tokens', 'NNS'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(\"There is some text to split into tokens.\")\n",
    "nltk.tag.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В nltk версии 3.2.5 есть поддержка теггера для русского языка, который использует тегсет НКРЯ. Сначала нужно загрузить соответствующую модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]     /Users/rhubarb/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_ru is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger_ru')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Эти', 'A-PRO=pl'),\n",
       " ('типы', 'S'),\n",
       " ('стали', 'V'),\n",
       " ('есть', 'V'),\n",
       " ('в', 'PR'),\n",
       " ('цехе', 'S'),\n",
       " ('.', 'NONLEX')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(\"Эти типы стали есть в цехе.\")\n",
    "nltk.tag.pos_tag(tokens, lang='rus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Теггеры из модуля nltk.tag**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве обучающих данных будем использовать Брауновский корпус. Загружаем теггер:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "utagger = nltk.tag.UnigramTagger(nltk.corpus.brown.tagged_sents(categories=\"news\", tagset='universal'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разметим предложение полученной моделью:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('You', 'PRON'),\n",
       " ('will', 'VERB'),\n",
       " ('benefit', 'NOUN'),\n",
       " ('from', 'ADP'),\n",
       " ('it', 'PRON')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utagger.tag(['You', 'will', 'benefit', 'from', 'it'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно оценить обученную модель на другом подкорпусе:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8282618852937741"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utagger.evaluate(nltk.corpus.brown.tagged_sents(categories=\"fiction\", tagset='universal'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем то же самое с биграммным теггером:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19635556593855857"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "btagger = nltk.tag.BigramTagger(nltk.corpus.brown.tagged_sents(categories=\"news\", tagset='universal'))\n",
    "# ...\n",
    "btagger.evaluate(nltk.corpus.brown.tagged_sents(categories=\"fiction\", tagset='universal'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Теггер Брилла](https://en.wikipedia.org/wiki/Brill_tagger) - модель, которая позволяет на основе статистики встречаемости тегов и контекстов в корпусе формулировать правила снятия морфологической неоднозначности. Сначала посмотрим на шаблоны этих правил:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Template(Pos([-1])),\n",
       " Template(Pos([1])),\n",
       " Template(Pos([-2])),\n",
       " Template(Pos([2])),\n",
       " Template(Pos([-2, -1])),\n",
       " Template(Pos([1, 2])),\n",
       " Template(Pos([-3, -2, -1])),\n",
       " Template(Pos([1, 2, 3])),\n",
       " Template(Pos([-1]),Pos([1])),\n",
       " Template(Pos([-2]),Pos([-1])),\n",
       " Template(Pos([1]),Pos([2])),\n",
       " Template(Word([-1])),\n",
       " Template(Word([1])),\n",
       " Template(Word([-2])),\n",
       " Template(Word([2])),\n",
       " Template(Word([-2, -1])),\n",
       " Template(Word([1, 2])),\n",
       " Template(Word([-1, 0])),\n",
       " Template(Word([0, 1])),\n",
       " Template(Word([0])),\n",
       " Template(Word([-1]),Pos([-1])),\n",
       " Template(Word([1]),Pos([1])),\n",
       " Template(Word([0]),Word([-1]),Pos([-1])),\n",
       " Template(Word([0]),Word([1]),Pos([1]))]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "templates = nltk.tag.brill.brill24()\n",
    "templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для теггера Брилла нужен теггер, который будет порождать исходную разметку. Возьмём уже обученный униграммный теггер и обучим теггер Брилла с некоторым заданным максимальным количеством правил:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9086117276019157"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brill_tagger = nltk.tag.BrillTaggerTrainer(utagger, templates)\n",
    "trained_tagger = brill_tagger.train(nltk.corpus.brown.tagged_sents(categories=\"religion\", tagset=\"universal\"), \n",
    "                                    max_rules=20)\n",
    "trained_tagger.evaluate(nltk.corpus.brown.tagged_sents(categories=\"fiction\", tagset=\"universal\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А так выведем получившиеся правила в виде\n",
    "id\\_шаблона исходная\\_чр "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Rule('006', None, 'NOUN', [(Pos([-3, -2, -1]),'DET')]),\n",
       " Rule('007', None, 'NOUN', [(Pos([1, 2, 3]),'.')]),\n",
       " Rule('007', None, 'NOUN', [(Pos([1, 2, 3]),'ADP')]),\n",
       " Rule('007', None, 'NOUN', [(Pos([1, 2, 3]),'VERB')]),\n",
       " Rule('001', 'PRT', 'ADP', [(Pos([1]),'DET')]),\n",
       " Rule('007', None, 'VERB', [(Pos([1, 2, 3]),'NOUN')]),\n",
       " Rule('000', 'NOUN', 'VERB', [(Pos([-1]),'PRON')]),\n",
       " Rule('017', 'NOUN', 'VERB', [(Word([-1, 0]),'to')]),\n",
       " Rule('019', 'ADJ', 'ADV', [(Word([0]),'only')]),\n",
       " Rule('019', 'NOUN', 'ADJ', [(Word([0]),'human')]),\n",
       " Rule('001', 'PRT', 'ADP', [(Pos([1]),'PRON')]),\n",
       " Rule('019', 'ADP', 'PRT', [(Word([0]),'all')]),\n",
       " Rule('009', 'NOUN', 'VERB', [(Pos([-2]),'VERB'), (Pos([-1]),'ADV')]),\n",
       " Rule('010', 'NOUN', 'ADJ', [(Pos([1]),'NOUN'), (Pos([2]),'ADP')]),\n",
       " Rule('018', 'NOUN', 'PRON', [(Word([0, 1]),'Him')]),\n",
       " Rule('008', 'VERB', 'NOUN', [(Pos([-1]),'DET'), (Pos([1]),'ADP')]),\n",
       " Rule('018', 'ADJ', 'ADV', [(Word([0, 1]),'more')]),\n",
       " Rule('011', 'NOUN', 'VERB', [(Word([-1]),'be')]),\n",
       " Rule('010', 'VERB', 'NOUN', [(Pos([1]),'CONJ'), (Pos([2]),'NOUN')]),\n",
       " Rule('010', 'NOUN', 'VERB', [(Pos([1]),'PRON'), (Pos([2]),'ADP')]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_tagger.rules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# requires pycrfsuite package\n",
    "\n",
    "# crf_tagger = nltk.tag.CRFTagger()\n",
    "# crf_tagger.train(nltk.corpus.brown.tagged_sents(categories=\"news\"))\n",
    "# crf_tagger.evaluate(nltk.corpus.brown.tagged_sents(categories=\"fiction\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# HMM tagger\n",
    "hmm_tagger = nltk.tag.hmm.HiddenMarkovModelTagger.train(nltk.corpus.brown.tagged_sents(categories=\"news\"))\n",
    "hmm_tagger.log_probability(['look/VB', 'forward/PP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# more taggers\n",
    "# Perceptron\n",
    "# SequentialTagger\n",
    "# --> NgramTagger\n",
    "# --> RegexpTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
